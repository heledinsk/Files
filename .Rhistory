sentiments <- data.frame(syuzhet, bing, afinn, nrc, Covid19Tweets$created_at)# the date is not be proper here if we sampled
sentiments <- sentiments %>%
arrange(Covid19Tweets.created_at)
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, Covid19Tweets$created_at)# the date is not be proper here if we sampled
View(Covid19Tweets)
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, Covid19Tweets$created)# the date is not be proper here if we sampled
tweetsdataset <- sample (Covid19Tweets, size=100)
### Prep
Covid19Tweets<- read_csv("C:/Users/avrch/Desktop/Files/BI AU - 2019-2021/2nd Semester/Applied Data Science/AppliedDataScience/Covid19Tweets.csv")
tweetsdataset <- sample (Covid19Tweets, size=100)
tweetsdataset <- tweetsdataset[sample(nrow(tweetsdataset), 100), ]
tweetsdataset <- Covid19Tweets[sample(nrow(tweetsdataset), 100), ]
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets), 100), ]
tweets <-tweetsdataset$text
#into character vecotr
tweets <- as.character(tweets) %>%
unique()
#using only sample size for faster performance
tweets <- sample (tweets, size=100)
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets), 100), ]%>%
unique()
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets), 100), ]%>%
unique()
tweets <-tweetsdataset$text
#into character vecotr
tweets <- as.character(tweets)
library(tm)
View(clean_tweets)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', tweets)
# remove @ people - this removes the whole string for person, do we want that or just delete the @
clean_tweets <- gsub('@\\w+', '', clean_tweets)
# remove punctuation
clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
# remove numbers
clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
# remove html links
clean_tweets = gsub('http\\w+', '', clean_tweets)
# remove unnecessary spaces
clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
# remove emojis or special characters
clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
# removing delimiters
clean_tweets <- removePunctuation(clean_tweets)
# removing whitespace
clean_tweets <- stripWhitespace(clean_tweets)
# lower case conversion
clean_tweets <- tolower(clean_tweets)
#(x)stopword removal ?? I skipped it for now - it does not work really :D
clean_tweets_stop <- removeWords(clean_tweets, stopwords("english"))
# stemming
clean_tweets_stem <- stemDocument(clean_tweets_stop)
# into tokens - not sure if useful, if we not decide to just use one word per tweet?
token <- words(clean_tweets_stem)
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(clean_tweets, method="syuzhet")
View(syuzhet)
bing <- get_sentiment(clean_tweets, method="bing")
afinn <- get_sentiment(clean_tweets, method="afinn")
nrc <- get_sentiment(clean_tweets, method="nrc")
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
sentiments <- sentiments %>%
arrange(Covid19Tweets.created_at)
# get the emotions using the NRC dictionary
emotions <- get_nrc_sentiment(clean_tweets)
View(emotions)
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
# get the emotions using the NRC dictionary
emotions <- get_nrc_sentiment(clean_tweets)
emo_bar = colSums(emotions)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
View(emo_sum)
#### Time series of sentiments over time ####
# plot the different sentiments from different methods in a time series
plot_ly(sentiments, x=~tweetsdataset.created, y=~syuzhet, type="scatter", mode="jitter", name="syuzhet") %>%
add_trace(y=~bing, mode="lines", name="bing") %>%
add_trace(y=~afinn, mode="lines", name="afinn") %>%
add_trace(y=~nrc, mode="lines", name="nrc") %>%
layout(title="Recent sentiments of HDB in Singapore",
yaxis=list(title="score"), xaxis=list(title="date"))
#### Time series of sentiments over time ####
# plot the different sentiments from different methods in a time series
plot_ly(sentiments, x=~tweetsdataset.created, y=~syuzhet, type="scatter", mode="jitter", name="syuzhet") %>%
add_trace(y=~bing, mode="lines", name="bing") %>%
add_trace(y=~afinn, mode="lines", name="afinn") %>%
add_trace(y=~nrc, mode="lines", name="nrc") %>%
layout(title="Recent sentiments regarding Covid19 in the UK",
yaxis=list(title="score"), xaxis=list(title="date"))
# Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""), showlegend=FALSE,
title="Distribution of emotion categories COVID19 in UK")
all = c(
paste(clean_tweets[emotions$anger > 0], collapse=" "),
paste(clean_tweets[emotions$anticipation > 0], collapse=" "),
paste(clean_tweets[emotions$disgust > 0], collapse=" "),
paste(clean_tweets[emotions$fear > 0], collapse=" "),
paste(clean_tweets[emotions$joy > 0], collapse=" "),
paste(clean_tweets[emotions$sadness > 0], collapse=" "),
paste(clean_tweets[emotions$surprise > 0], collapse=" "),
paste(clean_tweets[emotions$trust > 0], collapse=" ")
)
all <- removeWords(clean_tweets, stopwords("english"))
# create corpus
corpus = Corpus(VectorSource(all))
#
# create term-document matrix
tdm = TermDocumentMatrix(corpus)
#
# convert as matrix
tdm = as.matrix(tdm)
tdm1 <- tdm[nchar(rownames(tdm)) < 11,]
#
# add column names
colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')
#Hashtag column
hash <- str_match_all(sandbox$text, "#\\w+")
Covid19Tweets_hash <- Covid19Tweets %>%
mutate(hashtags = hash)
#Hashtag column
hash <- str_match_all(Covid19Tweets$text, "#\\w+")
Covid19Tweets_hash <- Covid19Tweets %>%
mutate(hashtags = hash)
#refer column
refer <- str_match_all(Covid19Tweets$text, "@\\w+")
Covid19Tweets_refer <- Covid19Tweets %>%
mutate(Refer = refer)
View(Covid19Tweets_refer)
#emoji
import emoji
Covid19Tweets_reply<-Covid19Tweets[!(Covid19Tweets$replyToSN=="NA")]
Covid19Tweets_hash <- Covid19Tweets %>%
mutate(hashtags = hash) %>%
select(-2,-(13:16))
View(Covid19Tweets_hash)
Covid19Tweets <-Covid19Tweets %>%
select(-2,-(13:16))
Covid19Tweets_hash <- Covid19Tweets %>%
mutate(hashtags = hash)
View(Covid19Tweets)
#Separate reply tweets
Covid19Tweets_reply <- data.frame(A, B, C, D, E,F,G,)
#Separate reply tweets
Covid19Tweets_reply <- data.frame(Covid19Tweets$text)
#Separate reply tweets
Covid19Tweets_reply <- data.frame(Covid19Tweets$text,Covid19Tweets$favoriteCount,Covid19Tweets$replyToSN,Covid19Tweets$created,Covid19Tweets$truncated, Covid19Tweets$replyToSID, Covid19Tweets$id,Covid19Tweets$replyToUID, Covid19Tweets$statusSource)
#Separate reply tweets
Covid19Tweets_reply <- data.frame(Covid19Tweets$text,Covid19Tweets$favoriteCount,Covid19Tweets$replyToSN,Covid19Tweets$created,Covid19Tweets$truncated, Covid19Tweets$replyToSID, Covid19Tweets$id,Covid19Tweets$replyToUID, Covid19Tweets$statusSource, Covid19Tweets$screenName, Covid19Tweets$retweetCount)
Covid19Tweets_reply<-Covid19Tweets[!(Covid19Tweets$replyToSN=="NA")]
Covid19Tweets <-Covid19Tweets %>%
select(-2,-(13:16))
### Prep
Covid19Tweets<- read_csv("C:/Users/avrch/Desktop/Files/BI AU - 2019-2021/2nd Semester/Applied Data Science/AppliedDataScience/Covid19Tweets.csv")
Covid19Tweets <-Covid19Tweets %>%
select(-2,-(13:16))
#Separate reply tweets
Covid19Tweets_reply <- data.frame(Covid19Tweets$text,Covid19Tweets$favoriteCount,Covid19Tweets$replyToSN,Covid19Tweets$created,Covid19Tweets$truncated, Covid19Tweets$replyToSID, Covid19Tweets$id,Covid19Tweets$replyToUID, Covid19Tweets$statusSource, Covid19Tweets$screenName, Covid19Tweets$retweetCount)
View(Covid19Tweets_reply)
Covid19Tweets_reply<-Covid19Tweets[!(Covid19Tweets.replyToSN=="NA")]
Covid19Tweets_reply<-Covid19Tweets[!(Covid19Tweets_reply$Covid19Tweets.replyToSN=="NA")]
Covid19Tweets_reply$Covid19Tweets.replyToSN
Covid19Tweets_reply<-Covid19Tweets[!is.na(Covid19Tweets_reply$Covid19Tweets.replyToSN)]
Covid19Tweets_reply <- completeFun(Covid19Tweets, "replyToSN")
install.packages("completeFun")
completeFun <- function(Covid19Tweets, replyToSN) {
completeVec <- complete.cases(Covid19Tweets[, replyToSN])
return(Covid19Tweets[completeVec, ])
}
Covid19Tweets_reply <- completeFun(Covid19Tweets, "replyToSN")
View(Covid19Tweets_reply)
Covid19Tweets_reply
View(Covid19Tweets_reply)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', Covid19Tweets_reply)
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets_reply), 100), ]%>%
unique() %>%
tweets <-tweetsdataset$text
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets_reply), 100), ]%>%
unique() %>%
tweets <-tweetsdataset$text
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets_reply), 100), ]%>%
unique() %>%
tweets <-tweetsdataset$text
#into character vector
tweets <- as.character(tweets)
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets_reply), 100), ]%>%
unique() %>%
tweets <-tweetsdataset$text
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets_reply), 100), ]%>%
unique()
tweets <-tweetsdataset$text
#into character vector
tweets <- as.character(tweets)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', tweets)
# remove @ people - this removes the whole string for person, do we want that or just delete the @
clean_tweets <- gsub('@\\w+', '', clean_tweets)
# remove punctuation
clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
# remove numbers
clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
# remove html links
clean_tweets = gsub('http\\w+', '', clean_tweets)
# remove unnecessary spaces
clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
# remove emojis or special characters
clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
# removing delimiters
clean_tweets <- removePunctuation(clean_tweets)
# removing whitespace
clean_tweets <- stripWhitespace(clean_tweets)
# lower case conversion
clean_tweets <- tolower(clean_tweets)
#(x)stopword removal ?? I skipped it for now - it does not work really :D
clean_tweets_stop <- removeWords(clean_tweets, stopwords("english"))
# stemming
clean_tweets_stem <- stemDocument(clean_tweets_stop)
# into tokens - not sure if useful, if we not decide to just use one word per tweet?
token <- words(clean_tweets_stem)
view(token)
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(clean_tweets, method="syuzhet")
bing <- get_sentiment(clean_tweets, method="bing")
afinn <- get_sentiment(clean_tweets, method="afinn")
nrc <- get_sentiment(clean_tweets, method="nrc")
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
# get the emotions using the NRC dictionary
emotions <- get_nrc_sentiment(clean_tweets)
emo_bar = colSums(emotions)
View(emo_bar)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
View(emo_sum)
#### Time series of sentiments over time ####
# plot the different sentiments from different methods in a time series
plot_ly(sentiments, x=~tweetsdataset.created, y=~syuzhet, type="scatter", mode="jitter", name="syuzhet") %>%
add_trace(y=~bing, mode="lines", name="bing") %>%
add_trace(y=~afinn, mode="lines", name="afinn") %>%
add_trace(y=~nrc, mode="lines", name="nrc") %>%
layout(title="Recent sentiments regarding COVID19 in the UK",
yaxis=list(title="score"), xaxis=list(title="date"))
#### Time series of sentiments over time ####
# plot the different sentiments from different methods in a time series
plot_ly(sentiments, x=~tweetsdataset.created, y=~syuzhet, type="scatter", mode="jitter", name="syuzhet") %>%
add_trace(y=~bing, mode="lines", name="bing") %>%
add_trace(y=~afinn, mode="lines", name="afinn") %>%
add_trace(y=~nrc, mode="lines", name="nrc") %>%
layout(title="Recent sentiments on replies regarding COVID19 in the UK",
yaxis=list(title="score"), xaxis=list(title="date"))
# Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""), showlegend=FALSE,
title="Distribution of emotion categories COVID19 in UK")
# Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""), showlegend=FALSE,
title="Distribution of emotion categories on replies COVID19 in UK")
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets), 100), ]%>%
unique() %>%
tweets <-tweetsdataset$text
#into character vector
tweets <- as.character(tweets)
library(tm)
View(clean_tweets)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', Covid19Tweets_reply)
# remove @ people - this removes the whole string for person, do we want that or just delete the @
clean_tweets <- gsub('@\\w+', '', clean_tweets)
# remove punctuation
clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
# remove numbers
clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
# remove html links
clean_tweets = gsub('http\\w+', '', clean_tweets)
# remove unnecessary spaces
clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
# remove emojis or special characters
clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
# removing delimiters
clean_tweets <- removePunctuation(clean_tweets)
# removing whitespace
clean_tweets <- stripWhitespace(clean_tweets)
# lower case conversion
clean_tweets <- tolower(clean_tweets)
#(x)stopword removal ?? I skipped it for now - it does not work really :D
clean_tweets_stop <- removeWords(clean_tweets, stopwords("english"))
# remove numbers
clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
# remove html links
clean_tweets = gsub('http\\w+', '', clean_tweets)
# remove unnecessary spaces
clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
# remove emojis or special characters
clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
# removing delimiters
clean_tweets <- removePunctuation(clean_tweets)
# removing whitespace
clean_tweets <- stripWhitespace(clean_tweets)
# lower case conversion
clean_tweets <- tolower(clean_tweets)
#(x)stopword removal ?? I skipped it for now - it does not work really :D
clean_tweets_stop <- removeWords(clean_tweets, stopwords("english"))
# stemming
clean_tweets_stem <- stemDocument(clean_tweets_stop)
# into tokens - not sure if useful, if we not decide to just use one word per tweet?
token <- words(clean_tweets_stem)
view(token)
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(clean_tweets, method="syuzhet")
bing <- get_sentiment(clean_tweets, method="bing")
afinn <- get_sentiment(clean_tweets, method="afinn")
nrc <- get_sentiment(clean_tweets, method="nrc")
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(clean_tweets, method="syuzhet")
### Prep
Covid19Tweets<- read_csv("C:/Users/avrch/Desktop/Files/BI AU - 2019-2021/2nd Semester/Applied Data Science/AppliedDataScience/Covid19Tweets.csv")
#Removing urls from trancated tweets
Covid19Tweets$text <- gsub("?http(s?)://(.*)", "", Covid19Tweets$text) #replacing urls with "" using regular expressions.
Covid19Tweets <-Covid19Tweets %>%
select(-2,-(13:16))
#Hashtag column
hash <- str_match_all(Covid19Tweets$text, "#\\w+")
#using only sample size for faster performance
tweetsdataset <- Covid19Tweets[sample(nrow(Covid19Tweets), 100), ]%>%
unique()
tweets <-tweetsdataset$text
#into character vector
tweets <- as.character(tweets)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', tweetsdataset)
# remove @ people - this removes the whole string for person, do we want that or just delete the @
clean_tweets <- gsub('@\\w+', '', clean_tweets)
# remove punctuation
clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
# remove numbers
clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
# remove html links
clean_tweets = gsub('http\\w+', '', clean_tweets)
# remove unnecessary spaces
clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
# remove emojis or special characters
clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
# removing delimiters
clean_tweets <- removePunctuation(clean_tweets)
# removing whitespace
clean_tweets <- stripWhitespace(clean_tweets)
# lower case conversion
clean_tweets <- tolower(clean_tweets)
#(x)stopword removal ?? I skipped it for now - it does not work really :D
clean_tweets_stop <- removeWords(clean_tweets, stopwords("english"))
# stemming
clean_tweets_stem <- stemDocument(clean_tweets_stop)
# into tokens - not sure if useful, if we not decide to just use one word per tweet?
token <- words(clean_tweets_stem)
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(clean_tweets, method="syuzhet")
View(syuzhet)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', tweetsdataset)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', tweets)
# remove retweet entities
clean_tweets <- gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', tweets)
# remove @ people - this removes the whole string for person, do we want that or just delete the @
clean_tweets <- gsub('@\\w+', '', clean_tweets)
# remove punctuation
clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
# remove numbers
clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
# remove html links
clean_tweets = gsub('http\\w+', '', clean_tweets)
# remove unnecessary spaces
clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
# remove emojis or special characters
clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
# removing delimiters
clean_tweets <- removePunctuation(clean_tweets)
# removing whitespace
clean_tweets <- stripWhitespace(clean_tweets)
# lower case conversion
clean_tweets <- tolower(clean_tweets)
#(x)stopword removal ?? I skipped it for now - it does not work really :D
clean_tweets_stop <- removeWords(clean_tweets, stopwords("english"))
# stemming
clean_tweets_stem <- stemDocument(clean_tweets_stop)
clean_tweets_stem
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(clean_tweets, method="syuzhet")
bing <- get_sentiment(clean_tweets, method="bing")
afinn <- get_sentiment(clean_tweets, method="afinn")
nrc <- get_sentiment(clean_tweets, method="nrc")
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
# get the emotions using the NRC dictionary
emotions <- get_nrc_sentiment(clean_tweets)
emo_bar = colSums(emotions)
sentiments
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(tweets,syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
sentiments
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
sentiments
View(sentiments)
#collect them in a data frame together with the tweet time (maybe also TweetID)
sentiments <- data.frame(tweets,syuzhet, bing, afinn, nrc, tweetsdataset$created)# the date is not be proper here if we sampled
View(sentiments)
sentiments <- sentiments %>%
arrange(tweetsdataset.created)
# get the emotions using the NRC dictionary
emotions <- get_nrc_sentiment(clean_tweets)
View(emotions)
sentiments
View(sentiments)
# get the emotions using the NRC dictionary
emotions <- get_nrc_sentiment(clean_tweets)
View(emotions)
emo_bar = colSums(emotions)
View(emo_bar)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
View(emo_sum)
#### Time series of sentiments over time ####
# plot the different sentiments from different methods in a time series
plot_ly(sentiments, x=~tweetsdataset.created, y=~syuzhet, type="scatter", mode="jitter", name="syuzhet") %>%
add_trace(y=~bing, mode="lines", name="bing") %>%
add_trace(y=~afinn, mode="lines", name="afinn") %>%
add_trace(y=~nrc, mode="lines", name="nrc") %>%
layout(title="Recent sentiments regarding COVID19 in the UK",
yaxis=list(title="score"), xaxis=list(title="date"))
# Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""), showlegend=FALSE,
title="Distribution of emotion categories COVID19 in UK")
all = c(
paste(clean_tweets[emotions$anger > 0], collapse=" "),
paste(clean_tweets[emotions$anticipation > 0], collapse=" "),
paste(clean_tweets[emotions$disgust > 0], collapse=" "),
paste(clean_tweets[emotions$fear > 0], collapse=" "),
paste(clean_tweets[emotions$joy > 0], collapse=" "),
paste(clean_tweets[emotions$sadness > 0], collapse=" "),
paste(clean_tweets[emotions$surprise > 0], collapse=" "),
paste(clean_tweets[emotions$trust > 0], collapse=" ")
)
all <- removeWords(clean_tweets, stopwords("english"))
all
# create corpus
corpus = Corpus(VectorSource(all))
#
# create term-document matrix
tdm = TermDocumentMatrix(corpus)
#
# convert as matrix
tdm = as.matrix(tdm)
tdm1 <- tdm[nchar(rownames(tdm)) < 11,]
#
# add column names
colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')
tdm
View(tdm1)
#
# add column names
colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')
colnames(tdm1) <- colnames(tdm)
comparison.cloud(tdm1, random.order=FALSE,
colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"),
title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4)
completeFun <- function(Covid19Tweets, replyToSN) {
completeVec <- complete.cases(Covid19Tweets[, replyToSN])
return(Covid19Tweets[completeVec, ])
}
Covid19Tweets_reply <- completeFun(Covid19Tweets, "replyToSN")
Covid19Tweets_reply <- !completeFun(Covid19Tweets, "replyToSN")
Covid19Tweets_reply <- Covid19Tweets[-completeFun(Covid19Tweets, "replyToSN")]
Covid19Tweets_reply <- completeFun(Covid19Tweets, "replyToSN")
withoutreplies <- Covid19Tweets[-Covid19Tweets_reply]
